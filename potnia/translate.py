# import re
# from typing import List
# import yaml

# # Adapted from https://github.com/j-luo93/NeuroDecipher/blob/master/notebooks/Linear_b_simplified.ipynb


# from potnia.mapping import AEGEAN_SYLLABOGRAMS_TO_SYLLABLE

# breakpoint()

# LINEAR_A_SYLLABOGRAMS_TO_SYLLABLE = dict(AEGEAN_SYLLABOGRAMS_TO_SYLLABLE)

# LINEAR_B_SYLLABOGRAMS_TO_SYLLABLE = dict(AEGEAN_SYLLABOGRAMS_TO_SYLLABLE)
# LINEAR_B_SYLLABOGRAMS_TO_SYLLABLE.update({
#     u'ðƒ': 'dwe', u'ð„': 'dwo', u'ð…': 'nwa', u'ð†': 'pu2', 
#     u'ð‡': 'pte', u'ðˆ': 'ra2', u'ð‰': 'ra3', u'ðŠ': 'ro2', u'ð‹': 'ta2', u'ðŒ': 'twe', u'ð': 'two'
# })



# LINEAR_B_TO_SYLLABLE = {
#     u'ð€€': 'a', u'ð€': 'e', u'ð€‚': 'i', u'ð€ƒ': 'o', u'ð€„': 'u', u'ð€…': 'da', u'ð€†': 'de', 
#     u'ð€‡': 'di', u'ð€ˆ': 'do', u'ð€‰': 'du', u'ð€Š': 'ja', u'ð€‹': 'je', u'ð€': 'jo', 
#     u'ð€Ž': 'ju', u'ð€': 'ka', u'ð€': 'ke', u'ð€‘': 'ki', u'ð€’': 'ko', u'ð€“': 'ku', 
#     u'ð€”': 'ma', u'ð€•': 'me', u'ð€–': 'mi', u'ð€—': 'mo', u'ð€˜': 'mu', u'ð€™': 'na', 
#     u'ð€š': 'ne', u'ð€›': 'ni', u'ð€œ': 'no', u'ð€': 'nu', u'ð€ž': 'pa', u'ð€Ÿ': 'pe', 
#     u'ð€ ': 'pi', u'ð€¡': 'po', u'ð€¢': 'pu', u'ð€£': 'qa', u'ð€¤': 'qe', u'ð€¥': 'qi', 
#     u'ð€¦': 'qo', u'ð€¨': 'ra', u'ð€©': 're', u'ð€ª': 'ri', u'ð€«': 'ro', u'ð€¬': 'ru',
#     u'ð€­': 'sa', u'ð€®': 'se', u'ð€¯': 'si', u'ð€°': 'so', u'ð€±': 'su', u'ð€²': 'ta', 
#     u'ð€³': 'te', u'ð€´': 'ti', u'ð€µ': 'to', u'ð€¶': 'tu', u'ð€·': 'wa', u'ð€¸': 'we', 
#     u'ð€¹': 'wi', u'ð€º': 'wo', u'ð€¼': 'za', u'ð€½': 'ze', u'ð€¿': 'zo', u'ð€': 'a2', 
#     u'ð': 'a3', u'ð‚': 'au', u'ðƒ': 'dwe', u'ð„': 'dwo', u'ð…': 'nwa', u'ð†': 'pu2', 
#     u'ð‡': 'pte', u'ðˆ': 'ra2', u'ð‰': 'ra3', u'ðŠ': 'ro2', u'ð‹': 'ta2', u'ðŒ': 'twe', u'ð': 'two'
# }

# SYLLABLE_TO_LINEAR_A
# SYLLABLE_TO_LINEAR_B = {v: k for k, v in LINEAR_B_TO_SYLLABLE.items()}


# # Define a function to tokenize text data from Linear B documents
# def tokenize_linear_b(text:str) -> List[str]:
#     """
#     Tokenizes Linear B text into individual elements, including words and special characters.
    
#     Parameters:
#     text (str): A string containing Linear B text to be tokenized.
    
#     Returns:
#     list: A list of tokens extracted from the text.
#     """
#     # Remove \u0323 from the text
#     text = text.replace('\u0323', '')

#     # Define a regex pattern for special characters treated as separate tokens, including \u27e6 and \u27e7
#     special_chars_pattern = r'(\[|\]|\,|\'|\u27e6|\u27e7)'
#     # Split the text on spaces to get initial tokens
#     tokens = text.split()

#     # Initialize a list to store final tokenized elements
#     tokenized = []

#     # Process each token for further splitting
#     for token in tokens:
#         # Split the token based on special characters
#         split_tokens = re.split(special_chars_pattern, token)
#         # Further split tokens containing hyphens
#         for tok in split_tokens:
#             if tok:  # Ensure token is not empty
#                 if '-' in tok:
#                     # Split on hyphen and include it as a separate token
#                     hyphen_split = re.split('(-)', tok)
#                     tokenized.extend(hyphen_split)
#                 else:
#                     tokenized.append(tok)

#     # Filter out empty tokens
#     tokenized = [tok for tok in tokenized if tok and tok != '-']

#     return tokenized


# def tokenize_linear_a(text:str) -> List[str]:
#     """
#     Tokenizes the input text according to specific rules:
#     - '[?]' is treated as a single token
#     - '[' and ']' are individual tokens
#     - '-' is a token on its own, except when part of '[?]'
#     - Regular words and characters are tokenized based on these delimiters
    
#     Parameters:
#         text (str): The text to tokenize.
    
#     Returns:
#         list[str]: A list of tokens following the specified rules.
#     """
#     # Define a regular expression pattern to match '[?]', '[', ']', and '-',
#     # along with sequences of characters that do not include these special characters
#     pattern = re.compile(r'\[\?\]|[\[\]-]|[^[\]\-]+')
#     tokens = pattern.findall(text)
#     return [token for token in tokens if token != "-"]


# def to_linear_b_word(latin_transliterated_word:str) -> str:
#     syllables = tokenize_linear_b(latin_transliterated_word)
#     symbols = [SYLLABLE_TO_LINEAR_B[syllable] for syllable in syllables]
#     return ''.join(symbols)


# def to_linear_b(latin_transliterated:str) -> str:
#     """ Adapted from https://github.com/j-luo93/NeuroDecipher/blob/master/notebooks/Linear_b_simplified.ipynb """
#     words = re.split(r"\s+", latin_transliterated)
#     words = [to_linear_b_word(word) for word in words]
#     result = ' '.join(words)
#     return result


# def to_linear_a_word(latin_transliterated_word:str) -> str:
#     syllables = tokenize_linear_a(latin_transliterated_word)
#     symbols = [SYLLABLE_TO_LINEAR_A[syllable] for syllable in syllables]
#     return ''.join(symbols)


# def to_linear_a(latin_transliterated:str) -> str:
#     words = re.split(r"\s+", latin_transliterated)
#     words = [to_linear_a_word(word) for word in words]
#     result = ' '.join(words)
#     return result


